{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Toxic Comment_25836_25824.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BahIx8mgRnAK"
      },
      "source": [
        "#colab\n",
        "# google-drive-ocamlfuseのインストール\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "# Colab用のAuth token作成\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Drive FUSE library用のcredential生成\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "# drive/ を作り、そこにGoogle Driveをマウントする\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqyRA4uCtwRN"
      },
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTyHq3hXXXbP"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import csv\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YmBzSp6Xxvw"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNKVXXUsYCSU"
      },
      "source": [
        "split data set on train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLv_LHuAX6Ey"
      },
      "source": [
        "train = pd.read_csv('drive/app/ToxicComments/train.csv', quotechar='\"').fillna(' ')\n",
        "\n",
        "# print train.describe()\n",
        "\n",
        "#razdelqme gi na train i test (~20%)\n",
        "test = train.iloc[130000:,:]\n",
        "train = train.iloc[:130000,:]\n",
        "train_text = train['comment_text']\n",
        "test_text = test['comment_text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCXoA9p7_gwz"
      },
      "source": [
        "len(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbvEp3LhYNRr"
      },
      "source": [
        "1grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV-NhWcxRvUm"
      },
      "source": [
        "word_vectorizer = TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='word',\n",
        "    token_pattern=r'\\w{1,}',\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 1),\n",
        "    max_features=10000)\n",
        "word_vectorizer.fit(train_text)\n",
        "\n",
        "train_features = word_vectorizer.transform(train_text)\n",
        "test_features = word_vectorizer.transform(test_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVYlS-0BHK5W"
      },
      "source": [
        "train_features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASVqSNsJYTZo"
      },
      "source": [
        "word_vectorizer2 = TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='word',\n",
        "    token_pattern=r'\\w{1,}',\n",
        "    stop_words='english',\n",
        "    ngram_range=(2, 2),\n",
        "    max_features=10000)\n",
        "word_vectorizer2.fit(train_text)\n",
        "\n",
        "train_features_2grams = word_vectorizer2.transform(train_text)\n",
        "test_features_2grams = word_vectorizer2.transform(test_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzbD1Dq4YVUw"
      },
      "source": [
        "word_vectorizer3 = TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='word',\n",
        "    token_pattern=r'\\w{1,}',\n",
        "    stop_words='english',\n",
        "    ngram_range=(3, 3),\n",
        "    max_features=10000)\n",
        "word_vectorizer3.fit(train_text)\n",
        "\n",
        "train_features_3grams = word_vectorizer3.transform(train_text)\n",
        "test_features_3grams = word_vectorizer3.transform(test_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBoV-rv5ZT5k"
      },
      "source": [
        "#Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ddM-zOoZqpd"
      },
      "source": [
        "st= time.time()\n",
        "scores = []\n",
        "for class_name in class_names:\n",
        "    train_target = train[class_name]\n",
        "    classifier = LogisticRegression(C=0.1, solver='sag')\n",
        "\n",
        "    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='accuracy'))\n",
        "    scores.append(cv_score)\n",
        "    print 'Score for class '+str(class_name)+': '+str(cv_score)\n",
        "\n",
        "    classifier.fit(train_features, train_target)\n",
        "    \n",
        "print('Metrics for 1gram')\n",
        "print ('Total score: '+str(np.mean(scores)))\n",
        "print ('Total time: '+str(int(time.time()-st)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88lYCaqrZ1Uj"
      },
      "source": [
        "st= time.time()\n",
        "scores = []\n",
        "for class_name in class_names:\n",
        "    train_target = train[class_name]\n",
        "    classifier = LogisticRegression(C=0.1, solver='sag')\n",
        "\n",
        "    cv_score = np.mean(cross_val_score(classifier, train_features_2grams , train_target , cv=3, scoring='accuracy'))\n",
        "    scores.append(cv_score)\n",
        "    print 'Score for class '+str(class_name)+': '+str(cv_score)\n",
        "\n",
        "    classifier.fit(train_features_2grams, train_target)\n",
        "    \n",
        "print('Metrics for _2grams ')\n",
        "print ('Total score: '+str(np.mean(scores)))\n",
        "print ('Total time: '+str(int(time.time()-st)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64M-ArFFaQwZ"
      },
      "source": [
        "st= time.time()\n",
        "scores = []\n",
        "for class_name in class_names:\n",
        "    train_target = train[class_name]\n",
        "    classifier = LogisticRegression(C=0.1, solver='sag')\n",
        "\n",
        "    cv_score = np.mean(cross_val_score(classifier, train_features_3grams, train_target, cv=3, scoring='accuracy'))\n",
        "    scores.append(cv_score)\n",
        "    print 'Score for class '+str(class_name)+': '+str(cv_score)\n",
        "\n",
        "    classifier.fit(train_features_3grams, train_target)\n",
        "    \n",
        "print('Metrics for _3grams ')\n",
        "print ('Total score: '+str(np.mean(scores)))\n",
        "print ('Total time: '+str(int(time.time()-st)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZekz5QZcItY"
      },
      "source": [
        "#Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IixKXD8ncHd5"
      },
      "source": [
        "st= time.time()\n",
        "scores = []\n",
        "for class_name in class_names:\n",
        "    train_target = train[class_name]\n",
        "    classifier = MultinomialNB()\n",
        "\n",
        "    cv_score = np.mean(cross_val_score(classifier, train_features.toarray(), train_target.as_matrix(), cv=3, scoring='accuracy'))\n",
        "    scores.append(cv_score)\n",
        "    print 'Score for class '+str(class_name)+': '+str(cv_score)\n",
        "\n",
        "    classifier.fit(train_features.toarray(), train_target.as_matrix())\n",
        "    \n",
        "print('Metrics for _1grams ')\n",
        "print ('Total score: '+str(np.mean(scores)))\n",
        "print ('Total time: '+str(int(time.time()-st)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmz9OOmCgPax"
      },
      "source": [
        "st= time.time()\n",
        "scores = []\n",
        "for class_name in class_names:\n",
        "    train_target = train[class_name]\n",
        "    classifier = MultinomialNB()\n",
        "\n",
        "    cv_score = np.mean(cross_val_score(classifier, train_features_2grams.toarray(), train_target.as_matrix(), cv=3, scoring='accuracy'))\n",
        "    scores.append(cv_score)\n",
        "    print 'Score for class '+str(class_name)+': '+str(cv_score)\n",
        "\n",
        "    classifier.fit(train_features_2grams.toarray(), train_target.as_matrix())\n",
        "    \n",
        "print('Metrics for _2grams ')\n",
        "print ('Total score: '+str(np.mean(scores)))\n",
        "print ('Total time: '+str(int(time.time()-st)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgx182VZjge5"
      },
      "source": [
        "st= time.time()\n",
        "scores = []\n",
        "for class_name in class_names:\n",
        "    train_target = train[class_name]\n",
        "    classifier = MultinomialNB()\n",
        "\n",
        "    cv_score = np.mean(cross_val_score(classifier, train_features_3grams.toarray(), train_target.as_matrix(), cv=3, scoring='accuracy'))\n",
        "    scores.append(cv_score)\n",
        "    print 'Score for class '+str(class_name)+': '+str(cv_score)\n",
        "\n",
        "    classifier.fit(train_features_3grams.toarray(), train_target.as_matrix())\n",
        "    \n",
        "print('Metrics for _3grams ')\n",
        "print ('Total score: '+str(np.mean(scores)))\n",
        "print ('Total time: '+str(int(time.time()-st)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2c_Ur1Xj_Wk"
      },
      "source": [
        "#Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tcQZKTUkB_i"
      },
      "source": [
        "st= time.time()\n",
        "scores = []\n",
        "for class_name in class_names:\n",
        "    train_target = train[class_name]\n",
        "    classifier = RandomForestClassifier(n_estimators=50, max_depth=50, random_state=1)\n",
        "\n",
        "    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='accuracy'))\n",
        "    scores.append(cv_score)\n",
        "    print 'Score for class '+str(class_name)+': '+str(cv_score)\n",
        "\n",
        "    classifier.fit(train_features, train_target)\n",
        "\n",
        "print('Metrics for _1grams ')\n",
        "print 'Total score: '+str(np.mean(scores))\n",
        "print 'Total time: '+str(int(time.time()-st))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE3rNMJKkHkU"
      },
      "source": [
        "st= time.time()\n",
        "scores = []\n",
        "for class_name in class_names:\n",
        "    train_target = train[class_name]\n",
        "    classifier = RandomForestClassifier(n_estimators=50, max_depth=50, random_state=1)\n",
        "\n",
        "    cv_score = np.mean(cross_val_score(classifier, train_features_2grams, train_target, cv=3, scoring='accuracy'))\n",
        "    scores.append(cv_score)\n",
        "    print 'Score for class '+str(class_name)+': '+str(cv_score)\n",
        "\n",
        "    classifier.fit(train_features_2grams, train_target)\n",
        "\n",
        "print('Metrics for _2grams ')\n",
        "print 'Total score: '+str(np.mean(scores))\n",
        "print 'Total time: '+str(int(time.time()-st))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8sPIQtFkOer"
      },
      "source": [
        "st= time.time()\n",
        "scores = []\n",
        "for class_name in class_names:\n",
        "    train_target = train[class_name]\n",
        "    classifier = RandomForestClassifier(n_estimators=50, max_depth=50, random_state=1)\n",
        "\n",
        "    cv_score = np.mean(cross_val_score(classifier, train_features_3grams, train_target, cv=3, scoring='accuracy'))\n",
        "    scores.append(cv_score)\n",
        "    print 'Score for class '+str(class_name)+': '+str(cv_score)\n",
        "\n",
        "    classifier.fit(train_features_2grams, train_target)\n",
        "\n",
        "print('Metrics for _3grams ')\n",
        "print 'Total score: '+str(np.mean(scores))\n",
        "print 'Total time: '+str(int(time.time()-st))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSmgMofkRs5"
      },
      "source": [
        "#Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ihvfcVEkWUT"
      },
      "source": [
        "st= time.time()\n",
        "scores = []\n",
        "for class_name in class_names:\n",
        "    train_target = train[class_name]\n",
        "    classifier = MLPClassifier(solver='adam', learning_rate='adaptive', hidden_layer_sizes=(50, 5), random_state=1, batch_size=5000) #, max_iter=100)\n",
        "\n",
        "    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='accuracy'))\n",
        "    scores.append(cv_score)\n",
        "    print 'Score for class '+str(class_name)+': '+str(cv_score)\n",
        "\n",
        "    classifier.fit(train_features, train_target)\n",
        "print('Metrics for _1grams ')\n",
        "print 'Total score: '+str(np.mean(scores))\n",
        "print 'Total time: '+str(int(time.time()-st))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz5I0nbgkcfB"
      },
      "source": [
        "st= time.time()\n",
        "scores = []\n",
        "for class_name in class_names:\n",
        "    train_target = train[class_name]\n",
        "    classifier = MLPClassifier(solver='adam', learning_rate='adaptive', hidden_layer_sizes=(50, 5), random_state=1, batch_size=5000) #, max_iter=100)\n",
        "\n",
        "    cv_score = np.mean(cross_val_score(classifier, train_features_2grams, train_target, cv=3, scoring='accuracy'))\n",
        "    scores.append(cv_score)\n",
        "    print 'Score for class '+str(class_name)+': '+str(cv_score)\n",
        "\n",
        "    classifier.fit(train_features_2grams, train_target)\n",
        "print('Metrics for _2grams ')\n",
        "print 'Total score: '+str(np.mean(scores))\n",
        "print 'Total time: '+str(int(time.time()-st))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1cZS26CkdML"
      },
      "source": [
        "st= time.time()\n",
        "scores = []\n",
        "for class_name in class_names:\n",
        "    train_target = train[class_name]\n",
        "    classifier = MLPClassifier(solver='adam', learning_rate='adaptive', hidden_layer_sizes=(50, 5), random_state=1, batch_size=5000) #, max_iter=100)\n",
        "\n",
        "    cv_score = np.mean(cross_val_score(classifier, train_features_3grams, train_target, cv=3, scoring='accuracy'))\n",
        "    scores.append(cv_score)\n",
        "    print 'Score for class '+str(class_name)+': '+str(cv_score)\n",
        "\n",
        "    classifier.fit(train_features_3grams, train_target)\n",
        "print('Metrics for _3grams ')\n",
        "print 'Total score: '+str(np.mean(scores))\n",
        "print 'Total time: '+str(int(time.time()-st))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJMIAaAlnu49"
      },
      "source": [
        "#Convolution Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL-kZqJ4CwZU"
      },
      "source": [
        "import keras\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9tbccEkkRFF"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from collections import defaultdict\n",
        "from keras.layers.convolutional import Convolution1D\n",
        "from keras import backend as K\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgU0XSjdi8VR"
      },
      "source": [
        "\n",
        "max_features=10000\n",
        "nb_filter = 250\n",
        "filter_length = 3\n",
        "hidden_dims = 250\n",
        "nb_epoch = 1\n",
        "nb_classes=6\n",
        "batch_size=200\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9GrfEdqkQLM"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from collections import defaultdict\n",
        "from keras.layers.convolutional import Convolution1D\n",
        "from keras import backend as K\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1lxtOs8i9H3"
      },
      "source": [
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128, dropout=0.2))\n",
        "# we add a Convolution1D, which will learn nb_filter\n",
        "# word group filters of size filter_length:\n",
        "model.add(Convolution1D(nb_filter=nb_filter,\n",
        "                        filter_length=filter_length,\n",
        "                        border_mode='valid',\n",
        "                        activation='relu',\n",
        "                        subsample_length=1))\n",
        "def max_1d(X):\n",
        "    return K.max(X, axis=1)\n",
        "\n",
        "model.add(Lambda(max_1d, output_shape=(nb_filter,)))#maks pooling\n",
        "model.add(Dense(hidden_dims)) \n",
        "model.add(Dropout(0.2)) \n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_classes))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDUKFDORnOfp"
      },
      "source": [
        "train_target=train.drop(['id','comment_text'], axis=1)\n",
        "test_target=test.drop(['id','comment_text'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHegOw2umiLA"
      },
      "source": [
        "st= time.time()\n",
        "print('Train...')\n",
        "model.fit(train_features, train_target, batch_size=batch_size, nb_epoch=1,\n",
        "          validation_data=(test_features, test_target))\n",
        "score, acc = model.evaluate(test_features, test_target,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "print 'Total time: '+str(int(time.time()-st))\n",
        "\n",
        "print(\"Generating test predictions...\")\n",
        "preds = model.predict_classes(test_features, verbose=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8MVu-F2-kHW"
      },
      "source": [
        "print(\"Generating test predictions...\")\n",
        "preds = model.predict_classes(test_features, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}